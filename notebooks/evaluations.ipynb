{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluations\n",
    "\n",
    "This notebook is used to: \n",
    "\n",
    "1. ~~produce *baselines* for the GaussianProxy models (data vs data FID)~~ -> moved to `scripts/metrics_null_test.py`\n",
    "2. load metrics computed on generated data and plot them against the baselines\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import json\n",
    "import math\n",
    "import pickle\n",
    "import sys\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from IPython.display import HTML\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "from torchvision.transforms import Compose, ConvertImageDtype, RandomHorizontalFlip, RandomVerticalFlip\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sys.path.insert(0, \"..\")\n",
    "from GaussianProxy.utils.data import RandomRotationSquareSymmetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(context=\"paper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_conf.dataset.biotine_png_128_hard_aug_inference import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dataset.dataset_params is not None\n",
    "database_path = Path(dataset.path)\n",
    "print(f\"Using dataset {dataset.name} from {database_path}\")\n",
    "subdirs: list[Path] = [e for e in database_path.iterdir() if e.is_dir() and not e.name.startswith(\".\")]\n",
    "subdirs.sort(key=dataset.dataset_params.sorting_func)\n",
    "print(f\"Found {len(subdirs)} times: {subdirs}\")\n",
    "\n",
    "# now split the dataset into 2 non-overlapping parts, respecting classes proportions...\n",
    "# ...and repeat that 10 times to get std of the metric\n",
    "is_flip_or_rotation = lambda t: isinstance(t, (RandomHorizontalFlip, RandomVerticalFlip, RandomRotationSquareSymmetry))\n",
    "flips_rot = [t for t in dataset.transforms.transforms if is_flip_or_rotation(t)]\n",
    "\n",
    "# with or without augmentations:\n",
    "# transforms = Compose(flips_rot + [ConvertImageDtype(torch.uint8)])\n",
    "transforms = Compose([ConvertImageDtype(torch.uint8)])\n",
    "\n",
    "print(f\"Using transforms:\\n{transforms}\")\n",
    "\n",
    "\n",
    "def count_elements(subdir: Path):\n",
    "    return subdir.name, len(list(subdir.glob(f\"*.{dataset.dataset_params.file_extension}\")))\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = {executor.submit(count_elements, subdir): subdir for subdir in subdirs}\n",
    "    nb_elems_per_class = {}\n",
    "    for future in tqdm(concurrent.futures.as_completed(futures), total=len(subdirs), desc=\"Counting elements per time\"):\n",
    "        subdir_name, count = future.result()\n",
    "        nb_elems_per_class[subdir_name] = count\n",
    "nb_elems_per_class[\"all_classes\"] = sum(nb_elems_per_class.values())\n",
    "\n",
    "print(f\"Number of elements per class: {nb_elems_per_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_repeats = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train vs train (null test) FIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_test_path = Path(\"evaluations\", dataset.name, \"eval_metrics.json\")\n",
    "assert null_test_path.exists(), f\"File {null_test_path} does not exist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(null_test_path, \"r\") as f:\n",
    "    train_vs_train_eval_metrics = json.load(f)\n",
    "\n",
    "class_names = sorted(train_vs_train_eval_metrics[\"exp_rep_0\"].keys(), key=lambda x: int(x))\n",
    "fid_scores_by_class_train = {class_name: [] for class_name in class_names}\n",
    "\n",
    "for exp_rep in train_vs_train_eval_metrics.values():\n",
    "    for class_name in class_names:\n",
    "        fid_scores_by_class_train[class_name].append(exp_rep[class_name][\"frechet_inception_distance\"])\n",
    "\n",
    "print(\"FID scores by class for train vs train:\")\n",
    "pprint(fid_scores_by_class_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load gen vs train FIDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: from `inference.py` script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_FIDs_generation_path = Path(\n",
    "    \"/\",\n",
    "    \"projects\",\n",
    "    \"static2dynamic\",\n",
    "    \"Thomas\",\n",
    "    \"experiments\",\n",
    "    \"GaussianProxy\",\n",
    "    \"biotine_all_paired_new_jz_MANUAL_WEIGHTS_DOWNLOAD_FROM_JZ_11-02-2025_14h31\",  # <- change here\n",
    "    \"inferences\",\n",
    "    \"MetricsComputation_100_diffsteps_no_SNR_leading_bf16_fixed\",  # <- change here\n",
    "    \"all_procs_metrics_dict.pkl\",\n",
    ")\n",
    "assert saved_FIDs_generation_path.exists(), f\"File {saved_FIDs_generation_path} does not exist\"\n",
    "experiment_name = saved_FIDs_generation_path.parts[6]\n",
    "inference_name = saved_FIDs_generation_path.parts[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract FID scores on generated data vs training data\n",
    "with open(saved_FIDs_generation_path, \"rb\") as f:\n",
    "    fid_scores_by_class_gen = pickle.load(f)\n",
    "pprint(fid_scores_by_class_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: load custom file, eg CSV from wandb logs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"tmp_downloaded_eval_values/wandb_export_2024-12-05T16_56_51.199+01_00.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"Step\"] == 120000]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_scores_by_class_gen = df[[c for c in df.columns if c.endswith(\"frechet_inception_distance\")]]\n",
    "col_names = [\"0.0003\", \"0.001\", \"0.003\", \"0.01\", \"0.03\", \"0.1\", \"0.3\", \"1.0\", \"all_classes\"]\n",
    "fid_scores_by_class_gen.columns = col_names\n",
    "fid_scores_by_class_gen = fid_scores_by_class_gen.iloc[0].to_dict()\n",
    "fid_scores_by_class_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, val in fid_scores_by_class_gen.items():\n",
    "    fid_scores_by_class_gen[key] = {\"frechet_inception_distance\": val}\n",
    "pprint(fid_scores_by_class_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6), dpi=300)\n",
    "class_labels = [f\"{class_name}\\n({nb_elems_per_class[class_name]})\" for class_name in class_names]\n",
    "# null-test\n",
    "plt.boxplot(\n",
    "    [fid_scores_by_class_train[class_name] for class_name in class_names],\n",
    "    tick_labels=class_labels,\n",
    "    showfliers=True,\n",
    "    flierprops=dict(marker=\"x\", markersize=3),\n",
    "    label=\"true data vs true data\",\n",
    ")\n",
    "# gen values\n",
    "y_gen = []\n",
    "for cl_name in class_names:\n",
    "    if cl_name in fid_scores_by_class_gen:\n",
    "        fid = fid_scores_by_class_gen[cl_name][\"frechet_inception_distance\"]\n",
    "        y_gen.append(fid)\n",
    "    else:\n",
    "        y_gen.append(np.nan)\n",
    "plt.scatter(\n",
    "    x=range(1, len(class_names) + 1),\n",
    "    y=y_gen,\n",
    "    label=\"generated data vs true data\",\n",
    ")\n",
    "plt.xlabel(\"Class Name (total number of class elements)\")\n",
    "plt.ylabel(\"FID Score\")\n",
    "plt.title(f\"Dataset: {dataset.name} | Experiment: {experiment_name}\")\n",
    "plt.suptitle(\"Intra-class FID score\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.grid(axis=\"x\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "# plt.ylim(2, 90.5)\n",
    "plt.figtext(0, 0.01, f\"Inference strategy: {inference_name}\", fontsize=8)\n",
    "plt.savefig(f\"evaluations/{dataset.name}/intra_class_fid_score_{experiment_name}_{inference_name}.png\")\n",
    "print(f\"Figure saved to evaluations/{dataset.name}/intra_class_fid_score_{experiment_name}_{inference_name}.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "# train\n",
    "corresp_x_values = []\n",
    "y_vals = []\n",
    "for cl_name in class_names:\n",
    "    corresp_x_values += [cl_name] * nb_repeats\n",
    "    y_vals += fid_scores_by_class_train[cl_name]\n",
    "fig.add_trace(\n",
    "    go.Box(\n",
    "        y=y_vals,\n",
    "        x=corresp_x_values,\n",
    "        name=\"train vs train\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# gen\n",
    "corresp_x_values = list(fid_scores_by_class_gen.keys())\n",
    "y_vals = [fid_scores_by_class_gen[cl_name][\"frechet_inception_distance\"] for cl_name in corresp_x_values]\n",
    "fig.add_trace(\n",
    "    go.Box(\n",
    "        y=y_vals,\n",
    "        x=corresp_x_values,\n",
    "        name=\"gen vs train\",\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    yaxis_title=\"FID Score\",\n",
    "    boxmode=\"group\",\n",
    "    xaxis_title=\"Class Name (total number of class elements)\",\n",
    "    title=\"Intra-class FID score\",\n",
    "    xaxis=dict(\n",
    "        tickmode=\"array\",\n",
    "        tickvals=list(range(len(class_names))),\n",
    "        ticktext=[f\"{class_name}<br>({nb_elems_per_class[class_name]})\" for class_name in class_names],\n",
    "        tickangle=-45,\n",
    "    ),\n",
    "    height=700,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot comparison of inference experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_experiment_names = [\n",
    "    \"MetricsComputation_100_diffsteps_no_SNR_leading_adapt_half_aug\",\n",
    "    \"MetricsComputation_100_diffsteps_with_SNR_trailing_adapt_half_aug\",\n",
    "    \"MetricsComputation_100_diffsteps_no_SNR_leading_f32\",\n",
    "    \"MetricsComputation_100_diffsteps_no_SNR_leading_bf16_fixed\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results = {}\n",
    "\n",
    "for exp_name in list_experiment_names:\n",
    "    parts = list(saved_FIDs_generation_path.parts)\n",
    "    parts[8] = exp_name\n",
    "    this_exp_path = Path(*parts)\n",
    "    assert this_exp_path.exists(), f\"File {this_exp_path} does not exist\"\n",
    "\n",
    "    # Extract FID scores on generated data vs training data\n",
    "    with open(this_exp_path, \"rb\") as f:\n",
    "        fid_scores_by_class_gen = pickle.load(f)\n",
    "    experiment_results[exp_name] = {\n",
    "        time: metrics[\"frechet_inception_distance\"] for time, metrics in fid_scores_by_class_gen.items()\n",
    "    }\n",
    "\n",
    "experiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6), dpi=300)\n",
    "class_labels = [f\"{class_name}\\n({nb_elems_per_class[class_name]})\" for class_name in class_names]\n",
    "# null-test\n",
    "plt.boxplot(\n",
    "    [fid_scores_by_class_train[class_name] for class_name in class_names],\n",
    "    tick_labels=class_labels,\n",
    "    showfliers=True,\n",
    "    flierprops=dict(marker=\"x\", markersize=3),\n",
    "    label=\"true data vs true data\",\n",
    ")\n",
    "# gen values\n",
    "for exp_name, exp_res in experiment_results.items():\n",
    "    y_gen = []\n",
    "    for cl_name in class_names:  # in order\n",
    "        if cl_name in fid_scores_by_class_gen:\n",
    "            fid = exp_res[cl_name]\n",
    "            y_gen.append(fid)\n",
    "        else:\n",
    "            y_gen.append(np.nan)\n",
    "    plt.scatter(\n",
    "        x=range(1, len(class_names) + 1),\n",
    "        y=y_gen,\n",
    "        label=f\"generated data vs true data - {exp_name}\",\n",
    "        marker=\"x\",\n",
    "    )\n",
    "plt.xlabel(\"Class Name (total number of class elements)\")\n",
    "plt.ylabel(\"FID Score\")\n",
    "plt.title(f\"Dataset: {dataset.name} | Experiment: {experiment_name}\")\n",
    "plt.suptitle(\"Intra-class FID score\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.grid(axis=\"x\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"evaluations/{dataset.name}/intra_class_fid_score_{experiment_name}_{inference_name}.png\")\n",
    "print(f\"Figure saved to evaluations/{dataset.name}/intra_class_fid_score_{experiment_name}_{inference_name}.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare reconstructions to original data of different scheduler configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp: compute L2\n",
    "from PIL import Image\n",
    "\n",
    "base_path = Path(\n",
    "    \"/projects/static2dynamic/Thomas/experiments/GaussianProxy/biotine_all_paired_new_jz_MANUAL_WEIGHTS_DOWNLOAD_FROM_JZ_11-02-2025_14h31/inferences\"\n",
    ")\n",
    "\n",
    "experiment_1_path = (\n",
    "    base_path / \"InversionRegenerationOnly_test_scheduler_100_diffsteps_M_13_fld_3\" / \"regeneration_-1_1 raw.png\"\n",
    ")\n",
    "experiment_1 = Image.open(experiment_1_path)\n",
    "\n",
    "experiment_2_path = (\n",
    "    base_path / \"InversionRegenerationOnly_100_diffsteps_no_SNR_leading_f32\" / \"regeneration_-1_1 raw.png\"\n",
    ")\n",
    "experiment_2 = Image.open(experiment_2_path)\n",
    "\n",
    "reference_path = (\n",
    "    base_path / \"InversionRegenerationOnly_test_scheduler_100_diffsteps_M_13_fld_3\" / \"starting_samples_-1_1 raw.png\"\n",
    ")\n",
    "reference = Image.open(reference_path)\n",
    "\n",
    "experiment_1 = np.array(experiment_1)\n",
    "experiment_2 = np.array(experiment_2)\n",
    "reference = np.array(reference)\n",
    "\n",
    "print(f\"L2 between {experiment_1_path.parent.name} and reference: {np.linalg.norm(experiment_1 - reference)}\")\n",
    "print(f\"L2 between {experiment_2_path.parent.name} and reference: {np.linalg.norm(experiment_2 - reference)}\")\n",
    "print(\n",
    "    f\"L2 between {experiment_1_path.parent.name} and {experiment_2_path.parent.name}: {np.linalg.norm(experiment_1 - experiment_2)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Function to prepare image for display\n",
    "def prepare_image(img_array: np.ndarray | Image.Image, max_size: int = 128) -> np.ndarray:\n",
    "    if isinstance(img_array, np.ndarray):\n",
    "        img = Image.fromarray(img_array.astype(\"uint8\"))\n",
    "    else:\n",
    "        img = img_array\n",
    "    img.thumbnail((max_size, max_size), Image.LANCZOS)\n",
    "    return np.array(img)\n",
    "\n",
    "\n",
    "# Function to place images as nodes\n",
    "def add_image(ax: plt.Axes, img: np.ndarray | Image.Image, xy: tuple, zoom: float = 1.0) -> None:\n",
    "    img_prepared = prepare_image(img)\n",
    "    img_box = OffsetImage(img_prepared, zoom=zoom)\n",
    "    ab = AnnotationBbox(img_box, xy, frameon=True, pad=0.1, bboxprops=dict(edgecolor=\"white\", facecolor=\"white\"))\n",
    "    ax.add_artist(ab)\n",
    "\n",
    "\n",
    "# Get experiment names from paths\n",
    "exp1_name = experiment_1_path.parent.name\n",
    "exp2_name = experiment_2_path.parent.name\n",
    "ref_name = \"reference\"\n",
    "\n",
    "# Calculate L2 distances\n",
    "l2_no_zero_vs_ref = np.linalg.norm(experiment_1 - reference)\n",
    "l2_zero_vs_ref = np.linalg.norm(experiment_2 - reference)\n",
    "l2_no_zero_vs_zero = np.linalg.norm(experiment_1 - experiment_2)\n",
    "\n",
    "# Normalize distances to fit within plotting area\n",
    "scale_factor = 1.0 / max(l2_no_zero_vs_ref, l2_zero_vs_ref, l2_no_zero_vs_zero)\n",
    "scaled_dist_1 = l2_no_zero_vs_ref * scale_factor\n",
    "scaled_dist_2 = l2_zero_vs_ref * scale_factor\n",
    "scaled_dist_3 = l2_no_zero_vs_zero * scale_factor\n",
    "\n",
    "# Position nodes using proportional distances\n",
    "# Place first two nodes\n",
    "pos_1 = np.array([0, 0])  # no_zero_SNR_leading\n",
    "pos_2 = np.array([scaled_dist_3, 0])  # zero_SNR_trailing\n",
    "\n",
    "\n",
    "# Function to calculate the position of the third point\n",
    "def get_third_point(pos_1: np.ndarray, pos_2: np.ndarray, d1: float, d2: float) -> np.ndarray:\n",
    "    \"\"\"Find position of third point given distances d1 and d2 from pos_1 and pos_2\"\"\"\n",
    "    # Using law of cosines to find the angle\n",
    "    cos_angle = (d1**2 + d2**2 - np.sum((pos_2 - pos_1) ** 2)) / (2 * d1 * d2)\n",
    "    cos_angle = min(1, max(-1, cos_angle))  # Ensure within valid range\n",
    "    angle = math.acos(cos_angle)\n",
    "\n",
    "    # Distance from pos_1\n",
    "    x = d1 * math.cos(angle)\n",
    "    y = d1 * math.sin(angle)\n",
    "\n",
    "    return np.array([x, y]) + pos_1\n",
    "\n",
    "\n",
    "# Find position for the reference point\n",
    "pos_3 = get_third_point(pos_1, pos_2, scaled_dist_1, scaled_dist_2)\n",
    "\n",
    "# Create positions dictionary with dynamic names from paths\n",
    "positions = {exp1_name: pos_1, exp2_name: pos_2, ref_name: pos_3}\n",
    "\n",
    "# Create figure and axes\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "# Add images at vertices\n",
    "add_image(ax, experiment_1, positions[exp1_name])\n",
    "add_image(ax, experiment_2, positions[exp2_name])\n",
    "add_image(ax, reference, positions[ref_name])\n",
    "\n",
    "# Draw lines representing distances\n",
    "for p1, p2, dist in [\n",
    "    (exp1_name, ref_name, l2_no_zero_vs_ref),\n",
    "    (exp2_name, ref_name, l2_zero_vs_ref),\n",
    "    (exp1_name, exp2_name, l2_no_zero_vs_zero),\n",
    "]:\n",
    "    ax.plot([positions[p1][0], positions[p2][0]], [positions[p1][1], positions[p2][1]], \"k-\", alpha=0.7)\n",
    "\n",
    "    # Add distance labels\n",
    "    midpoint = ((positions[p1][0] + positions[p2][0]) / 2, (positions[p1][1] + positions[p2][1]) / 2)\n",
    "    ax.annotate(\n",
    "        f\"L2 = {dist:.2e}\",\n",
    "        midpoint,\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.8),\n",
    "    )\n",
    "\n",
    "# Add node labels with adaptive positioning based on triangle shape\n",
    "offset = 0.15\n",
    "# Adjust label positions based on point locations\n",
    "for name, pos in positions.items():\n",
    "    if name == ref_name:\n",
    "        # Place above or below depending on y-coordinate\n",
    "        if pos[1] > 0:\n",
    "            ax.text(pos[0], pos[1] + offset, name, ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "        else:\n",
    "            ax.text(pos[0], pos[1] - offset, name, ha=\"center\", va=\"top\", fontsize=10)\n",
    "    else:\n",
    "        # For the bottom points\n",
    "        ax.text(pos[0], pos[1] - offset, name, ha=\"center\", va=\"top\", fontsize=10)\n",
    "\n",
    "# Set limits and turn off axes\n",
    "all_x = [p[0] for p in positions.values()]\n",
    "all_y = [p[1] for p in positions.values()]\n",
    "x_margin, y_margin = 0.5, 0.5\n",
    "ax.set_xlim(min(all_x) - x_margin, max(all_x) + x_margin)\n",
    "ax.set_ylim(min(all_y) - y_margin, max(all_y) + y_margin)\n",
    "ax.axis(\"off\")\n",
    "\n",
    "plt.title(\"Image Comparison Triangle with Proportional L2 Distances\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare histograms of true vs generated data along time\n",
    "\n",
    "(sanity check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chose experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_data_path = saved_FIDs_generation_path.parent\n",
    "print(f\"Using generated data at {gen_data_path}\")\n",
    "print(\"Project:\", gen_data_path.parts[-3])\n",
    "print(\"Inference experiment:\", gen_data_path.parts[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files_in_subdir(subdir: Path, extension: str) -> list[Path]:\n",
    "    files = []\n",
    "    ext = f\".{extension}\"\n",
    "\n",
    "    if not subdir.exists():\n",
    "        print(f\"Subdirectory {subdir} does not exist.\")\n",
    "        return files\n",
    "\n",
    "    for entry in subdir.iterdir():\n",
    "        if entry.is_file() and entry.suffix == ext:\n",
    "            files.append(entry)\n",
    "        elif entry.is_dir():\n",
    "            print(f\"Ignoring sub-subdirectory: {entry}\")\n",
    "\n",
    "    return files\n",
    "\n",
    "\n",
    "def parallel_file_search(base_path: Path, extension: str, subdirs: list[str]) -> list[Path]:\n",
    "    # Get all files in parallel\n",
    "    all_files = []\n",
    "    with ProcessPoolExecutor() as executor:\n",
    "        futures = {executor.submit(find_files_in_subdir, base_path / subdir, extension): subdir for subdir in subdirs}\n",
    "\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Searching directories\"):\n",
    "            all_files.extend(future.result())\n",
    "\n",
    "    return all_files\n",
    "\n",
    "\n",
    "common_times_to_use = set(class_names) & set(\n",
    "    d.name for d in gen_data_path.iterdir() if d.is_dir() and d.name in class_names\n",
    ")\n",
    "print(f\"Using common times: {common_times_to_use} from {class_names}\")\n",
    "\n",
    "\n",
    "def get_image_RGB_histogram(\n",
    "    image_path: Path, bins: int, expected_shape: tuple[int, int, int], expected_dtype: np.dtype\n",
    "):\n",
    "    image = Image.open(image_path)\n",
    "    image = np.array(image)\n",
    "    assert image.shape == expected_shape, (\n",
    "        f\"Expected shape {expected_shape}, but got {image.shape} for image {image_path}\"\n",
    "    )\n",
    "    assert image.dtype == expected_dtype, (\n",
    "        f\"Expected dtype {expected_dtype}, but got {image.dtype} for image {image_path}\"\n",
    "    )\n",
    "    r = image[:, :, 0]\n",
    "    g = image[:, :, 1]\n",
    "    b = image[:, :, 2]\n",
    "    r_hist, _ = np.histogram(r, bins=bins, range=(0, 255))\n",
    "    g_hist, _ = np.histogram(g, bins=bins, range=(0, 255))\n",
    "    b_hist, _ = np.histogram(b, bins=bins, range=(0, 255))\n",
    "    return r_hist, g_hist, b_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find generated files\n",
    "\n",
    "run this on each new inference experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated patches\n",
    "print(f\"Using generated dataset at {gen_data_path}\")\n",
    "print(\"Searching for generated files...\")\n",
    "all_gen_patches = parallel_file_search(gen_data_path, dataset.dataset_params.file_extension, common_times_to_use)\n",
    "print(f\"Found {len(all_gen_patches)} generated patches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute and save true data histograms\n",
    "\n",
    "run this only once to compute & save the true data histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True patches\n",
    "not_augmented_ds_path = database_path.with_name(database_path.name.rstrip(\"_hard_augmented\"))\n",
    "print(f\"Using true, not augmented dataset at {not_augmented_ds_path}\")\n",
    "print(\"Searching for true files...\")\n",
    "# all_true_patches = list(database_path.rglob(f\"*.{dataset.dataset_params.file_extension}\"))\n",
    "# all_true_patches = fast_find_files(str(database_path), dataset.dataset_params.file_extension)\n",
    "all_true_patches = parallel_file_search(database_path, dataset.dataset_params.file_extension, common_times_to_use)\n",
    "print(f\"Found {len(all_true_patches)} true patches\")\n",
    "\n",
    "assert len(all_gen_patches) == len(all_true_patches) // 2, \"Number of generated patches should be half of the true ones\"\n",
    "\n",
    "# Get & save true patches histograms\n",
    "true_tot_r_hist = {str(time): np.zeros(256) for time in common_times_to_use}\n",
    "true_tot_g_hist = {str(time): np.zeros(256) for time in common_times_to_use}\n",
    "true_tot_b_hist = {str(time): np.zeros(256) for time in common_times_to_use}\n",
    "\n",
    "futures = {}\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    for image_path in all_true_patches:\n",
    "        time = image_path.parent.name\n",
    "        assert time in common_times_to_use, f\"Time {time} not in common times to use\"\n",
    "        futures[executor.submit(get_image_RGB_histogram, image_path, 256, (255, 255, 3), np.dtype(np.uint8))] = time\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Computing true histograms\"):\n",
    "        time = futures[future]\n",
    "        r_hist, g_hist, b_hist = future.result()\n",
    "        # sum histograms\n",
    "        true_tot_r_hist[time] += r_hist\n",
    "        true_tot_g_hist[time] += g_hist\n",
    "        true_tot_b_hist[time] += b_hist\n",
    "\n",
    "# Pickle save histograms under 'evaluations/<dataset_name>/histograms/'\n",
    "Path(\"evaluations\", dataset.name, \"histograms\").mkdir(parents=True, exist_ok=True)\n",
    "with open(f\"evaluations/{dataset.name}/histograms/true_tot_r_hist.pkl\", \"wb\") as f:\n",
    "    pickle.dump(true_tot_r_hist, f)\n",
    "with open(f\"evaluations/{dataset.name}/histograms/true_tot_g_hist.pkl\", \"wb\") as f:\n",
    "    pickle.dump(true_tot_g_hist, f)\n",
    "with open(f\"evaluations/{dataset.name}/histograms/true_tot_b_hist.pkl\", \"wb\") as f:\n",
    "    pickle.dump(true_tot_b_hist, f)\n",
    "print(f\"Saved true histograms to evaluations/{dataset.name}/histograms/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute and save generated data histograms\n",
    "\n",
    "run this on each new inference experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get & save generated patches histograms\n",
    "gen_tot_r_hist = {str(time): np.zeros(256) for time in common_times_to_use}\n",
    "gen_tot_g_hist = {str(time): np.zeros(256) for time in common_times_to_use}\n",
    "gen_tot_b_hist = {str(time): np.zeros(256) for time in common_times_to_use}\n",
    "\n",
    "futures = {}\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    for image_path in all_gen_patches:\n",
    "        time = image_path.parent.name\n",
    "        assert time in common_times_to_use, f\"Time {time} not in common times to use\"\n",
    "        futures[executor.submit(get_image_RGB_histogram, image_path, 256, (128, 128, 3), np.dtype(np.uint8))] = time\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Computing gen histograms\"):\n",
    "        time = futures[future]\n",
    "        r_hist, g_hist, b_hist = future.result()\n",
    "        # sum histograms\n",
    "        gen_tot_r_hist[time] += r_hist\n",
    "        gen_tot_g_hist[time] += g_hist\n",
    "        gen_tot_b_hist[time] += b_hist\n",
    "\n",
    "# Pickle save histograms under 'evaluations/<dataset_name>/histograms/'\n",
    "Path(gen_data_path, \"histograms\").mkdir(exist_ok=True)\n",
    "with open(f\"{gen_data_path}/histograms/gen_tot_r_hist.pkl\", \"wb\") as f:\n",
    "    pickle.dump(gen_tot_r_hist, f)\n",
    "with open(f\"{gen_data_path}/histograms/gen_tot_g_hist.pkl\", \"wb\") as f:\n",
    "    pickle.dump(gen_tot_g_hist, f)\n",
    "with open(f\"{gen_data_path}/histograms/gen_tot_b_hist.pkl\", \"wb\") as f:\n",
    "    pickle.dump(gen_tot_b_hist, f)\n",
    "print(f\"Saved generated histograms to {gen_data_path}/histograms/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_histogram_animation(y_lims: tuple[float, float, float]) -> animation.FuncAnimation:\n",
    "    # Load histograms\n",
    "    with open(true_histogram_path / \"true_tot_r_hist.pkl\", \"rb\") as f:\n",
    "        true_r_hist = pickle.load(f)\n",
    "    with open(true_histogram_path / \"true_tot_g_hist.pkl\", \"rb\") as f:\n",
    "        true_g_hist = pickle.load(f)\n",
    "    with open(true_histogram_path / \"true_tot_b_hist.pkl\", \"rb\") as f:\n",
    "        true_b_hist = pickle.load(f)\n",
    "\n",
    "    # Load generated histograms\n",
    "    with open(gen_histogram_path / \"gen_tot_r_hist.pkl\", \"rb\") as f:\n",
    "        gen_r_hist = pickle.load(f)\n",
    "    with open(gen_histogram_path / \"gen_tot_g_hist.pkl\", \"rb\") as f:\n",
    "        gen_g_hist = pickle.load(f)\n",
    "    with open(gen_histogram_path / \"gen_tot_b_hist.pkl\", \"rb\") as f:\n",
    "        gen_b_hist = pickle.load(f)\n",
    "\n",
    "    # Get all time points\n",
    "    times = sorted(gen_r_hist.keys(), key=lambda x: int(x))\n",
    "\n",
    "    # Create figure and axes\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(30, 10), dpi=300)\n",
    "    fig.suptitle(\"RGB Histograms: True vs Generated\", fontsize=16)\n",
    "\n",
    "    # Normalize histograms for better visualization\n",
    "    def normalize_hist(hist: np.ndarray) -> np.ndarray:\n",
    "        return hist / np.sum(hist) if np.sum(hist) > 0 else hist\n",
    "\n",
    "    # Animation update function\n",
    "    def update(frame: int) -> list:\n",
    "        time = times[frame]\n",
    "\n",
    "        # Clear all axes\n",
    "        for row in axes:\n",
    "            for ax in row:\n",
    "                ax.clear()\n",
    "\n",
    "        # Remove any existing time text\n",
    "        # Find and remove text at the bottom position\n",
    "        for text in fig.texts:\n",
    "            if text._y < 0.05:  # Text near the bottom\n",
    "                text.remove()\n",
    "\n",
    "        # Set titles for columns\n",
    "        axes[0, 0].set_title(\"True Data\")\n",
    "        axes[0, 1].set_title(\"Generated Data\")\n",
    "        axes[0, 2].set_title(\"Difference\")\n",
    "\n",
    "        # Define x-axis (pixel values)\n",
    "        x = np.arange(256)\n",
    "\n",
    "        # Plot R channel\n",
    "        axes[0, 0].bar(x, normalize_hist(true_r_hist[time]), color=\"red\", alpha=0.7)\n",
    "        axes[0, 1].bar(x, normalize_hist(gen_r_hist[time]), color=\"red\", alpha=0.7)\n",
    "        axes[0, 2].bar(x, normalize_hist(true_r_hist[time]) - normalize_hist(gen_r_hist[time]), color=\"red\", alpha=0.7)\n",
    "        axes[0, 0].set_ylabel(\"R Channel\")\n",
    "\n",
    "        # Plot G channel\n",
    "        axes[1, 0].bar(x, normalize_hist(true_g_hist[time]), color=\"green\", alpha=0.7)\n",
    "        axes[1, 1].bar(x, normalize_hist(gen_g_hist[time]), color=\"green\", alpha=0.7)\n",
    "        axes[1, 2].bar(\n",
    "            x, normalize_hist(true_g_hist[time]) - normalize_hist(gen_g_hist[time]), color=\"green\", alpha=0.7\n",
    "        )\n",
    "        axes[1, 0].set_ylabel(\"G Channel\")\n",
    "\n",
    "        # Plot B channel\n",
    "        axes[2, 0].bar(x, normalize_hist(true_b_hist[time]), color=\"blue\", alpha=0.7)\n",
    "        axes[2, 1].bar(x, normalize_hist(gen_b_hist[time]), color=\"blue\", alpha=0.7)\n",
    "        axes[2, 2].bar(x, normalize_hist(true_b_hist[time]) - normalize_hist(gen_b_hist[time]), color=\"blue\", alpha=0.7)\n",
    "        axes[2, 0].set_ylabel(\"B Channel\")\n",
    "\n",
    "        # Set x-axis labels\n",
    "        for ax in axes[2, :]:\n",
    "            ax.set_xlabel(\"Pixel Value\")\n",
    "\n",
    "        # Adjust y-axis limits for better visualization\n",
    "        for row_index, row in enumerate(axes):\n",
    "            for ax in row:\n",
    "                ax.set_ylim(0, y_lims[row_index])\n",
    "\n",
    "        # Add text indicating the current time point\n",
    "        fig.text(0.5, 0.01, f\"Current time point: {time}\", ha=\"center\", fontsize=12)\n",
    "\n",
    "        fig.tight_layout(rect=(0, 0.03, 1, 0.95))  # Make room for title and text\n",
    "\n",
    "        return axes.flatten()\n",
    "\n",
    "    # Create animation\n",
    "    anim = animation.FuncAnimation(fig, update, frames=len(times), interval=1500, blit=False)\n",
    "\n",
    "    return anim\n",
    "\n",
    "\n",
    "true_histogram_path = Path(\"evaluations\", dataset.name, \"histograms\")\n",
    "gen_histogram_path = Path(gen_data_path, \"histograms\")\n",
    "\n",
    "# Create and display the animation\n",
    "anim = create_histogram_animation((0.1, 0.05, 0.25))\n",
    "\n",
    "# To save the animation (uncomment to use)\n",
    "save_path = Path(gen_histogram_path / \"histogram_animation.mp4\")\n",
    "anim.save(save_path, writer=\"ffmpeg\", fps=1, dpi=300)\n",
    "print(f\"Animation saved to {save_path}\")\n",
    "\n",
    "# Display in notebook\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Different splits vs only augs vs diff splits no augs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load aug FIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"/workspaces/biocomp/tboyer/sources/GaussianProxy/notebooks/evaluations/BBBC021_196_docetaxel/eval_metrics_TEST_REPS_WITH_AUGS.json\",\n",
    "    \"r\",\n",
    ") as f:\n",
    "    eval_augs_metrics = json.load(f)\n",
    "eval_augs_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_scores_by_class_train_augs_only: dict[str, list[float]] = {}\n",
    "\n",
    "for class_name in class_names:\n",
    "    fid_scores_by_class_train_augs_only[class_name] = [\n",
    "        eval_augs_metrics[str(idx)][class_name][\"frechet_inception_distance\"] for idx in range(nb_repeats)\n",
    "    ]\n",
    "\n",
    "fid_scores_by_class_train_augs_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load no augs FIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"/workspaces/biocomp/tboyer/sources/GaussianProxy/notebooks/evaluations/BBBC021_196_docetaxel/eval_metrics_TEST_REPS_NO_AUGS.json\",\n",
    "    \"r\",\n",
    ") as f:\n",
    "    eval_no_augs_metrics = json.load(f)\n",
    "eval_no_augs_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_scores_by_class_train_no_augs: dict[str, list[float]] = {}\n",
    "\n",
    "for class_name in class_names:\n",
    "    fid_scores_by_class_train_no_augs[class_name] = [\n",
    "        eval_no_augs_metrics[f\"exp_rep_{idx}\"][class_name][\"frechet_inception_distance\"] for idx in range(nb_repeats)\n",
    "    ]\n",
    "\n",
    "fid_scores_by_class_train_no_augs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load hard augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    \"/workspaces/biocomp/tboyer/sources/GaussianProxy/notebooks/evaluations/BBBC021_196_docetaxel/eval_metrics_TEST_HARD_AUGS.json\",\n",
    "    \"r\",\n",
    ") as f:\n",
    "    eval_hard_augs_metrics = json.load(f)\n",
    "eval_hard_augs_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid_scores_by_class_train_hard_augs: dict[str, list[float]] = {}\n",
    "\n",
    "for class_name in class_names:\n",
    "    fid_scores_by_class_train_hard_augs[class_name] = [\n",
    "        eval_hard_augs_metrics[f\"repeat_{idx}\"][class_name][\"frechet_inception_distance\"] for idx in range(nb_repeats)\n",
    "    ]\n",
    "\n",
    "fid_scores_by_class_train_hard_augs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6), dpi=300)\n",
    "\n",
    "n_classes = len(class_names)\n",
    "positions1 = np.arange(1, n_classes + 1) - 0.25\n",
    "positions2 = np.arange(1, n_classes + 1)\n",
    "positions3 = np.arange(1, n_classes + 1) + 0.25\n",
    "positions4 = positions2  # values are very different so ok to \"overlap\" on the x axis\n",
    "\n",
    "class_labels = [f\"{class_name}\\n({nb_elems_per_class[class_name]})\" for class_name in class_names]\n",
    "bar_width = 0.1\n",
    "text_offset_y = 0.1\n",
    "text_offset_x = 0.12\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    # First group (blue)\n",
    "    plt.scatter(\n",
    "        np.full_like(fid_scores_by_class_train[class_name], positions1[i]),\n",
    "        fid_scores_by_class_train[class_name],\n",
    "        alpha=0.3,\n",
    "        color=\"blue\",\n",
    "        s=20,\n",
    "        label=\"diff splits   | o-t-f augs\" if i == 0 else \"\",\n",
    "    )\n",
    "    median1 = np.median(fid_scores_by_class_train[class_name])\n",
    "    plt.hlines(median1, positions1[i] - bar_width, positions1[i] + bar_width, colors=\"blue\", alpha=0.8, linewidth=2)\n",
    "    plt.text(\n",
    "        positions1[i] + text_offset_x,\n",
    "        median1 + text_offset_y,\n",
    "        f\"{median1:.1f}\",\n",
    "        color=\"blue\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        alpha=0.8,\n",
    "        fontsize=6,\n",
    "    )\n",
    "\n",
    "    # Second group (orange)\n",
    "    plt.scatter(\n",
    "        np.full_like(fid_scores_by_class_train_augs_only[class_name], positions2[i]),\n",
    "        fid_scores_by_class_train_augs_only[class_name],\n",
    "        alpha=0.3,\n",
    "        color=\"orange\",\n",
    "        s=20,\n",
    "        label=\"same split | o-t-f augs\" if i == 0 else \"\",\n",
    "    )\n",
    "    median2 = np.median(fid_scores_by_class_train_augs_only[class_name])\n",
    "    plt.hlines(median2, positions2[i] - bar_width, positions2[i] + bar_width, colors=\"orange\", alpha=0.8, linewidth=2)\n",
    "    plt.text(\n",
    "        positions2[i] + text_offset_x,\n",
    "        median2 + text_offset_y,\n",
    "        f\"{median2:.1f}\",\n",
    "        color=\"orange\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        alpha=0.8,\n",
    "        fontsize=6,\n",
    "    )\n",
    "\n",
    "    # Third group (green)\n",
    "    plt.scatter(\n",
    "        np.full_like(fid_scores_by_class_train_no_augs[class_name], positions3[i]),\n",
    "        fid_scores_by_class_train_no_augs[class_name],\n",
    "        alpha=0.3,\n",
    "        color=\"green\",\n",
    "        s=20,\n",
    "        label=\"diff splits   | no augs\" if i == 0 else \"\",\n",
    "    )\n",
    "    median3 = np.median(fid_scores_by_class_train_no_augs[class_name])\n",
    "    plt.hlines(median3, positions3[i] - bar_width, positions3[i] + bar_width, colors=\"green\", alpha=0.8, linewidth=2)\n",
    "    plt.text(\n",
    "        positions3[i] + text_offset_x,\n",
    "        median3 + text_offset_y,\n",
    "        f\"{median3:.1f}\",\n",
    "        color=\"green\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        alpha=0.8,\n",
    "        fontsize=6,\n",
    "    )\n",
    "\n",
    "    # Fourth group (red)\n",
    "    plt.scatter(\n",
    "        np.full_like(fid_scores_by_class_train_hard_augs[class_name], positions4[i]),\n",
    "        fid_scores_by_class_train_hard_augs[class_name],\n",
    "        alpha=0.3,\n",
    "        color=\"red\",\n",
    "        s=20,\n",
    "        label=\"diff splits   | hard augs\" if i == 0 else \"\",\n",
    "    )\n",
    "    median4 = np.median(fid_scores_by_class_train_hard_augs[class_name])\n",
    "    plt.hlines(median4, positions4[i] - bar_width, positions4[i] + bar_width, colors=\"red\", alpha=0.8, linewidth=2)\n",
    "    plt.text(\n",
    "        positions4[i] + text_offset_x,\n",
    "        median4 + text_offset_y,\n",
    "        f\"{median4:.1f}\",\n",
    "        color=\"red\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        alpha=0.8,\n",
    "        fontsize=6,\n",
    "    )\n",
    "\n",
    "plt.xlabel(\"Class Name (total number of class elements)\")\n",
    "plt.ylabel(\"FID Score\")\n",
    "plt.title(\"Intra-class true data vs true data FID score\")\n",
    "plt.xticks(range(1, n_classes + 1), class_labels, rotation=45, ha=\"right\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
